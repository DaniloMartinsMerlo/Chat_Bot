import streamlit as st
import time
import requests
import json
import chromadb
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import os
import textwrap
import pandas as pd
from typing import Dict, Any, Tuple, List
from functools import lru_cache

# ================ CONFIGURAÃ‡Ã•ES INICIAIS ================

load_dotenv()
API_KEY = os.getenv("API_KEY")
DOCS_PATH = "dataset"

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}

MODEL = "z-ai/glm-4.5-air:free"

# ================ INICIALIZAÃ‡ÃƒO OTIMIZADA ================

@st.cache_resource
def initialize_embedder():
    return SentenceTransformer("all-MiniLM-L6-v2")

@st.cache_resource
def initialize_chroma():
    client = chromadb.Client()
    collection = client.get_or_create_collection(name="docs")
    return client, collection

embedder = initialize_embedder()
chroma_client, collection = initialize_chroma()

# ================ FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO OTIMIZADAS ================

def chunk_text(text: str, max_chars: int = 500) -> List[str]:
    return textwrap.wrap(text, max_chars, break_long_words=False, replace_whitespace=False)

def add_text_batch(doc_id_prefix: str, text: str):
    chunks = chunk_text(text)
    
    if not chunks:
        return
    
    embeddings = embedder.encode(chunks, show_progress_bar=False)
    
    documents = chunks
    embedding_list = [emb.tolist() for emb in embeddings]
    ids = [f"{doc_id_prefix}_{i}" for i in range(len(chunks))]
    
    collection.add(
        documents=documents,
        embeddings=embedding_list,
        ids=ids
    )

def load_txt(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def load_csv(path: str, filename: str):
    df = pd.read_csv(path, low_memory=False)
    text = df.head(1000).to_string() 
    add_text_batch(filename, text)

@st.cache_data
def check_documents_loaded():
    try:
        count = collection.count()
        return count > 0
    except:
        return False

def load_all_documents(docs_path: str, force_reload: bool = False):
    if not os.path.exists(docs_path):
        return
    
    if not force_reload and check_documents_loaded():
        return

    global chroma_client, collection
    chroma_client.delete_collection(name="docs")
    collection = chroma_client.get_or_create_collection(name="docs")

    files = os.listdir(docs_path)
    
    for filename in enumerate(files):
        full_path = os.path.join(docs_path, filename)

        if filename.endswith(".csv"):
            load_csv(full_path, filename)
        elif filename.endswith(".txt"):
            text = load_txt(full_path)
            add_text_batch(filename, text)
        
load_all_documents(DOCS_PATH)

# ================ OPENROUTER API OTIMIZADA ================

def call_openrouter(payload: Dict[str, Any], timeout: int = 60) -> Tuple[Dict[str, Any], requests.Response]:
    try:
        resp = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=HEADERS,
            json=payload, 
            timeout=timeout
        )

        body = resp.json()

        if resp.status_code != 200:
            error_message = body.get("error", {}).get("message", "Erro desconhecido na API.")
            raise Exception(f"Erro na API (Status {resp.status_code}): {error_message}")

        return body, resp
    
    except requests.exceptions.Timeout:
        raise Exception("Timeout na requisiÃ§Ã£o Ã  API. Tente novamente.")
    except ValueError:
        raise Exception(f"Erro na API: Resposta nÃ£o Ã© JSON. Status: {resp.status_code}")

# ================ LÃ“GICA DE RESPOSTA OTIMIZADA ================

@lru_cache(maxsize=100)
def get_cached_embedding(question: str):
    return embedder.encode([question])[0]

def rag_query(question: str) -> str:
    embedding = get_cached_embedding(question)

    results = collection.query(
        query_embeddings=[embedding.tolist()],
        n_results=10,
    )

    retrieved_text = "\n\n".join(results["documents"][0])

    prompt = f"""
        VocÃª Ã© o **Assistente de Auditoria de Compliance da Dunder Mifflin**, trabalhando para Toby Flenderson (RH).

        Sua missÃ£o Ã© analisar documentos da empresa (polÃ­ticas, e-mails, transaÃ§Ãµes) e responder perguntas investigativas com PRECISÃƒO e EVIDÃŠNCIAS.


        **SUAS CAPACIDADES DE ANÃLISE: FaÃ§a apenas aquelas solicitadas pelo usuÃ¡rio**

        **CONSULTAS SOBRE POLÃTICAS DE COMPLIANCE**
        - Responda dÃºvidas dos colaboradores sobre regras, limites e procedimentos
        - Cite trechos especÃ­ficos da polÃ­tica quando relevante
        - Seja claro, didÃ¡tico e completo

        **INVESTIGAÃ‡ÃƒO**
        - Vasculhe e-mails procurando evidÃªncias de conspiraÃ§Ã£o
        - Para CADA e-mail suspeito, liste:
            * Remetente â†’ DestinatÃ¡rio
            * Trecho especÃ­fico do e-mail
            * Por que Ã© evidÃªncia de conspiraÃ§Ã£o
        - ConclusÃ£o final: "SIM, hÃ¡ evidÃªncias" ou "NÃƒO, nÃ£o hÃ¡ evidÃªncias"

        **VIOLAÃ‡Ã•ES DIRETAS DE COMPLIANCE**
        - Identifique transaÃ§Ãµes que SOZINHAS violam as polÃ­ticas
        - Tipos de violaÃ§Ã£o:
            * Valores acima dos limites permitidos
            * Categorias proibidas/restritas
            * AprovaÃ§Ãµes ausentes quando obrigatÃ³rias
            * FrequÃªncia/padrÃ£o suspeito
        - Para CADA violaÃ§Ã£o, liste:
            * ID da transaÃ§Ã£o
            * FuncionÃ¡rio, valor, categoria
            * Regra especÃ­fica violada (cite a polÃ­tica)
            * Gravidade (baixa/mÃ©dia/alta)

        **FRAUDES COM CONTEXTO DE E-MAILS**
        - Correlacione e-mails com transaÃ§Ãµes para detectar fraudes combinadas
        - Procure por:
            * E-mails combinando desvios + transaÃ§Ã£o correspondente
            * Acordos para burlar polÃ­ticas + evidÃªncia nas transaÃ§Ãµes
            * PadrÃµes de conspiraÃ§Ã£o financeira entre funcionÃ¡rios
        - Para CADA fraude, forneÃ§a:
            * **E-mail:** [Remetente â†’ DestinatÃ¡rio, trecho]
            * **TransaÃ§Ã£o:** [ID, valor, categoria, funcionÃ¡rio]
            * **ConexÃ£o:** Como o e-mail comprova a fraude
            * **Gravidade:** baixa/mÃ©dia/alta


        **REGRAS IMPORTANTES:**

        Seja DETALHADO e forneÃ§a EVIDÃŠNCIAS CONCRETAS sempre
        Cite: trechos de polÃ­ticas, IDs de transaÃ§Ãµes, remetentes de e-mails
        Use formataÃ§Ã£o clara (tÃ³picos, negrito) para organizar informaÃ§Ãµes
        Se nÃ£o houver dados suficientes, seja honesto: "NÃ£o encontrei evidÃªncias dessa violaÃ§Ã£o nos documentos analisados."
        Analise TODOS os documentos recuperados, nÃ£o apenas alguns

        Nunca invente dados ou transaÃ§Ãµes que nÃ£o estÃ£o nos documentos
        NÃ£o faÃ§a suposiÃ§Ãµes sem evidÃªncias concretas


        **DOCUMENTOS RECUPERADOS:**
        {retrieved_text}


        **PERGUNTA DE INVESTIGAÃ‡ÃƒO:**
        {question}


        **Responda agora com base nos documentos, fornecendo evidÃªncias especÃ­ficas e organizadas:**"""

    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 1500,
    }

    body, _ = call_openrouter(payload, timeout=60)
    return body["choices"][0]["message"]["content"]

def general_query(question: str) -> str:

    prompt = f"""
        VocÃª Ã© o Assistente de Auditoria de Compliance da Dunder Mifflin,
        mas nesta resposta vocÃª deve **ignorar totalmente o modo de auditoria**.

        O usuÃ¡rio fez uma pergunta geral, NÃƒO relacionada a investigaÃ§Ã£o, compliance ou documentos.

        Responda de forma:
        - curta
        - direta
        - clara
        - sem listar regras completas de auditoria
        - sem iniciar processos investigativos

        Explique APENAS o que foi perguntado de maneira simples e profissional.

        PERGUNTA: {question}
    """

    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 100,
    }

    body, _ = call_openrouter(payload, timeout=30)
    return body["choices"][0]["message"]["content"]

@lru_cache(maxsize=50)
def classify_intent_cached(question: str) -> str:
    
    prompt = f"""
        VocÃª deve classificar a pergunta do usuÃ¡rio em APENAS uma palavra:
        - "general"
        - "compliance"

        REGRAS:
        1. Se a pergunta for sobre vocÃª, suas habilidades, como vocÃª funciona, o que Ã© capaz de fazer, limitaÃ§Ãµes ou qualquer dÃºvida METALINGUÃSTICA â†’ responda "general".
        2. SÃ³ classifique como "compliance" quando o usuÃ¡rio pedir para:
        - analisar documentos
        - investigar transaÃ§Ãµes
        - investigar e-mails
        - detectar violaÃ§Ãµes
        - explicar polÃ­ticas da empresa
        - executar qualquer tarefa investigativa do sistema de auditoria
        3. NÃ£o classifique perguntas gerais como compliance.

        PERGUNTA: {question}
        Responda APENAS: general ou compliance.
    """

    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 10,
    }

    try:
        body, _ = call_openrouter(payload, timeout=15)
        intent = body["choices"][0]["message"]["content"].strip().lower()
        return "compliance" if intent not in ["general"] else intent
    except Exception:
        return "compliance"

def get_assistant_response(question: str) -> str:
    intent = classify_intent_cached(question)
    
    if intent == "general":
        return general_query(question)
    else:
        return rag_query(question)

# ================ FUNÃ‡Ã•ES DE EXIBIÃ‡ÃƒO DO STREAMLIT ================

def stream_text(text: str):
    for char in text:
        yield char
        time.sleep(0.005)  

def user_text(input_text: str) -> str:
    st.session_state["messages"].append({
        "role": "user", 
        "content": input_text, 
        "avatar": "assets/thunderatz.png"
    })
    
    with st.chat_message("user", avatar="assets/thunderatz.png"):
        st.write(input_text)
    
    return input_text

def ia_response(response: str):
    st.session_state["messages"].append({
        "role": "assistant", 
        "content": response, 
        "avatar": "ğŸ¤–"
    })
    
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.write_stream(stream_text(response))

# ================ APLICAÃ‡ÃƒO STREAMLIT PRINCIPAL ================

st.set_page_config(
    page_title="Assistente de Auditoria - Dunder Mifflin",
    page_icon="ğŸ”",
    layout="wide"
)

st.title("ğŸ” Assistente de Auditoria de Compliance")
st.markdown("### Dunder Mifflin - Filial Scranton")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"], avatar=msg.get("avatar", "ğŸ¤–")):
        st.write(msg["content"])

input_box = st.chat_input(
    placeholder="Digite sua pergunta sobre compliance...",
    key="chat_input"
)

if input_box:
    user_text(input_box)
    
    with st.spinner("Analisando..."):
        try:
            content = get_assistant_response(input_box)
        except Exception as e:
            content = f"Desculpe, ocorreu um erro: {str(e)}"
            st.error(content)
    
    ia_response(content)
    st.rerun()